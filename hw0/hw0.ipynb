{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 10-714: Homework 0"
      ],
      "metadata": {
        "id": "_kI7TL0zTIfJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 173,
      "metadata": {
        "id": "Iz4ZkDgSS16Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d925e994-55c4-428c-d867-65ec9845b707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/10714\n",
            "fatal: destination path 'hw0' already exists and is not an empty directory.\n",
            "/content/drive/MyDrive/10714/hw0\n"
          ]
        }
      ],
      "source": [
        "# Code to set up the assignment\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/MyDrive/\n",
        "!mkdir -p 10714\n",
        "%cd /content/drive/MyDrive/10714\n",
        "!git clone https://github.com/dlsyscourse/hw0.git\n",
        "%cd /content/drive/MyDrive/10714/hw0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade --no-deps git+https://github.com/dlsyscourse/mugrade.git\n",
        "!pip3 install pybind11\n",
        "!pip3 install numdifftools"
      ],
      "metadata": {
        "id": "m6kUyI2uS76e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "672302e5-ec83-4eb3-c74b-3bee8d280369"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/dlsyscourse/mugrade.git\n",
            "  Cloning https://github.com/dlsyscourse/mugrade.git to /tmp/pip-req-build-lm1yeiev\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/dlsyscourse/mugrade.git /tmp/pip-req-build-lm1yeiev\n",
            "  Resolved https://github.com/dlsyscourse/mugrade.git to commit 98609ee80ee24bf278455b48aa8d06bd3f5d0430\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pybind11 in /usr/local/lib/python3.10/dist-packages (2.10.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numdifftools in /usr/local/lib/python3.10/dist-packages (0.9.41)\n",
            "Requirement already satisfied: numpy>=1.9 in /usr/local/lib/python3.10/dist-packages (from numdifftools) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.8 in /usr/local/lib/python3.10/dist-packages (from numdifftools) (1.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 1: A basic add function, and testing/autograding basics\n",
        "\n",
        "To illustrate the workflow of these assignments and the autograding system, we'll use a simple example of implementing an add function. Note that the commands run above will create the following structure in your 10714/hw0 directory\n",
        "\n",
        "```\n",
        "data/\n",
        "    train-images-idx3-ubyte.gz\n",
        "    train-labels-idx1-ubyte.gz\n",
        "    t10k-images-idx3-ubyte.gz\n",
        "    t10k-labels-idx1-ubyte.gz\n",
        "src/\n",
        "    simple_ml.py\n",
        "    simple_ml_ext.cpp\n",
        "tests/\n",
        "    test_simple_ml.py\n",
        "Makefile\n",
        "```"
      ],
      "metadata": {
        "id": "7YtbQicXTUyt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first homework question requires you to implement simple_ml.add() function (this trivial function is not used anywhere, it is just an example to get you used to the structure of the assignment). Looking at the src/simple_ml.py file, you will find the following function stub for the add() function.\n",
        "\n",
        "```python\n",
        "def add(x, y):\n",
        "    \"\"\" A trivial 'add' function you should implement to get used to the\n",
        "    autograder and submission system.  The solution to this problem is in\n",
        "    the homework notebook.\n",
        "\n",
        "    Args:\n",
        "        x (Python number or numpy array)\n",
        "        y (Python number or numpy array)\n",
        "\n",
        "    Return:\n",
        "        Sum of x + y\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "_MJYH_SVfhCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running local tests\n",
        "\n",
        "Now you will want to test to see if your code works, and if so, to submit it to the autograding system. Throughout this course, we are using standard tools for running unit tests on code, namely the pytest system. Once you've written the correct code in the src/simple_ml.py file, run the following command below."
      ],
      "metadata": {
        "id": "Gg1NsD6Ed-bh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -k \"add\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5EQjUsPoTGZV",
        "outputId": "e9f7e7b9-145a-495a-c722-e80fada2f04c"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 0.72s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question 2: Loading MNIST data\n",
        "\n",
        "Now that you're familiar with the autograding system, try it out on the next function you need to implement in the src/simple_ml.py file: the parse_mnist_data() function. Here is the function declaration from the file (we typically won't walk through this whole process again, but will do so here one more time).\n",
        "\n",
        "```python\n",
        "def parse_mnist(image_filename, label_filename):\n",
        "    \"\"\" Read an images and labels file in MNIST format.  See this page:\n",
        "    http://yann.lecun.com/exdb/mnist/ for a description of the file format.\n",
        "\n",
        "    Args:\n",
        "        image_filename (str): name of gzipped images file in MNIST format\n",
        "        label_filename (str): name of gzipped labels file in MNIST format\n",
        "\n",
        "    Returns:\n",
        "        Tuple (X,y):\n",
        "            X (numpy.ndarray[np.float32]): 2D numpy array containing the loaded\n",
        "                data.  The dimensionality of the data should be\n",
        "                (num_examples x input_dim) where 'input_dim' is the full\n",
        "                dimension of the data, e.g., since MNIST images are 28x28, it\n",
        "                will be 784.  Values should be of type np.float32, and the data\n",
        "                should be normalized to have a minimum value of 0.0 and a\n",
        "                maximum value of 1.0. The normalization should be applied uniformly\n",
        "                across the whole dataset, _not_ individual images.\n",
        "\n",
        "            y (numpy.ndarray[dtype=np.uint8]): 1D numpy array containing the\n",
        "                labels of the examples.  Values should be of type np.uint8 and\n",
        "                for MNIST will contain the values 0-9.\n",
        "    \"\"\"\n",
        "```"
      ],
      "metadata": {
        "id": "_gTFpLdteJ1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -k \"parse_mnist\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pnvjqzhScTej",
        "outputId": "e64f7cba-5906-4478-b6ae-bf9333cd9514"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 1.58s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 3: Softmax loss\n",
        "\n",
        "Implement the softmax (a.k.a. cross-entropy) loss as defined in `softmax_loss()` function in `src/simple_ml.py`.  Recall (hopefully this is review, but we'll also cover it in lecture on 9/1), that for a multi-class output that can take on values $y \\in \\{1,\\ldots,k\\}$, the softmax loss takes as input a vector of logits $z \\in \\mathbb{R}^k$, the true class $y \\in \\{1,\\ldots,k\\}$ returns a loss defined by\n",
        "\\begin{equation}\n",
        "\\ell_{\\mathrm{softmax}}(z, y) = \\log\\sum_{i=1}^k \\exp z_i - z_y.\n",
        "\\end{equation}\n",
        "\n",
        "Note that as described in its docstring, `softmax_loss()` takes a _2D array_ of logits (i.e., the $k$ dimensional logits for a batch of different samples), plus a corresponding 1D array of true labels, and should output the _average_ softmax loss over the entire batch.  Note that to do this correctly, you should _not_ use any loops, but do all the computation natively with numpy vectorized operations (to set expectations here, we should note for instance that our reference solution consists of a single line of code).\n",
        "\n",
        "Note that for \"real\" implementation of softmax loss you would want to scale the logits to prevent numerical overflow, but we won't worry about that here (the rest of the assignment will work fine even if you don't worry about this).  The code below runs the test cases.\n"
      ],
      "metadata": {
        "id": "XVIgIsjtfGkS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -k \"softmax_loss\""
      ],
      "metadata": {
        "id": "uL4LOb5JrcVo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a593879-00af-4c15-9325-be2e1e31fabb"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 1.47s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 4: Stochastic gradient descent for softmax regression\n",
        "\n",
        "In this question you will implement stochastic gradient descent (SGD) for (linear) softmax regression.  In other words, as discussed in lecture on 9/1, we will consider a hypothesis function that makes $n$-dimensional inputs to $k$-dimensional logits via the function\n",
        "\\begin{equation}\n",
        "h(x) = \\Theta^T x\n",
        "\\end{equation}\n",
        "where $x \\in \\mathbb{R}^n$ is the input, and $\\Theta \\in \\mathbb{R}^{n \\times k}$ are the model parameters.  Given a dataset $\\{(x^{(i)} \\in \\mathbb{R}^n, y^{(i)} \\in \\{1,\\ldots,k\\})\\}$, for $i=1,\\ldots,m$, the optimization problem associated with softmax regression is thus given by\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\minimize}{minimize}\n",
        "\\minimize_{\\Theta} \\; \\frac{1}{m} \\sum_{i=1}^m \\ell_{\\mathrm{softmax}}(\\Theta^T x^{(i)}, y^{(i)}).\n",
        "\\end{equation}\n",
        "\n",
        "Recall from class that the gradient of the linear softmax objective is given by\n",
        "\\begin{equation}\n",
        "\\nabla_\\Theta \\ell_{\\mathrm{softmax}}(\\Theta^T x, y) = x (z - e_y)^T\n",
        "\\end{equation}\n",
        "where\n",
        "\\begin{equation}\n",
        "\\DeclareMathOperator*{\\normalize}{normalize}\n",
        "z = \\frac{\\exp(\\Theta^T x)}{1^T \\exp(\\Theta^T x)} \\equiv \\normalize(\\exp(\\Theta^T x))\n",
        "\\end{equation}\n",
        "(i.e., $z$ is just the normalized softmax probabilities), and where $e_y$ denotes the $y$th unit basis, i.e., a vector of all zeros with a one in the $y$th position.\n",
        "\n",
        "We can also write this in the more compact notation we discussed in class.  Namely, if we let $X \\in \\mathbb{R}^{m \\times n}$ denote a design matrix of some $m$ inputs (either the entire dataset or a minibatch), $y \\in \\{1,\\ldots,k\\}^m$ a corresponding vector of labels, and overloading $\\ell_{\\mathrm{softmax}}$ to refer to the average softmax loss, then\n",
        "\\begin{equation}\n",
        "\\nabla_\\Theta \\ell_{\\mathrm{softmax}}(X \\Theta, y) = \\frac{1}{m} X^T (Z - I_y)\n",
        "\\end{equation}\n",
        "where\n",
        "\\begin{equation}\n",
        "Z = \\normalize(\\exp(X \\Theta)) \\quad \\mbox{(normalization applied row-wise)}\n",
        "\\end{equation}\n",
        "denotes the matrix of logits, and $I_y \\in \\mathbb{R}^{m \\times k}$ represents a concatenation of one-hot bases for the labels in $y$.\n",
        "\n",
        "Using these gradients, implement the `softmax_regression_epoch()` function, which runs a single epoch of SGD (one pass over a data set) using the specified learning rate / step size `lr` and minibatch size `batch`.  As described in the docstring, your function should modify the `Theta` array in-place.  After implementation, run the tests."
      ],
      "metadata": {
        "id": "WYUSuFKdxgFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -k \"softmax_regression_epoch and not cpp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yA1bFtGUz5zq",
        "outputId": "461d242c-4d33-4b90-f714-e31ae8dae764"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 1.69s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training MNIST with softmax regression\n",
        "\n",
        "Although it's not a part of the tests, now that you have written this code, you can also try training a full MNIST linear classifier using SGD.  For this you can use the `train_softmax()` function in the `src/simple_ml.py` file (we have already written this function for you, so you don't need to write it yourself, though you can take a look to see what it's doing).\n",
        "\n",
        "You can see how this works using the following code.  For reference, as seen below, our implementation runs in ~3 seconds on Colab, and achieves 7.97% error."
      ],
      "metadata": {
        "id": "qi_iXQN-Znk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"src/\")\n",
        "from simple_ml import train_softmax, parse_mnist\n",
        "\n",
        "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
        "                         \"data/train-labels-idx1-ubyte.gz\")\n",
        "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
        "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr=0.2, batch=100)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5wg8zfWZpEe",
        "outputId": "3e86c747-86d6-4911-ab03-9d4e019be129"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
            "|     0 |    0.35134 |   0.10182 |   0.33588 |  0.09400 |\n",
            "|     1 |    0.32142 |   0.09268 |   0.31086 |  0.08730 |\n",
            "|     2 |    0.30802 |   0.08795 |   0.30097 |  0.08550 |\n",
            "|     3 |    0.29987 |   0.08532 |   0.29558 |  0.08370 |\n",
            "|     4 |    0.29415 |   0.08323 |   0.29215 |  0.08230 |\n",
            "|     5 |    0.28981 |   0.08182 |   0.28973 |  0.08090 |\n",
            "|     6 |    0.28633 |   0.08085 |   0.28793 |  0.08080 |\n",
            "|     7 |    0.28345 |   0.07997 |   0.28651 |  0.08040 |\n",
            "|     8 |    0.28100 |   0.07923 |   0.28537 |  0.08010 |\n",
            "|     9 |    0.27887 |   0.07847 |   0.28442 |  0.07970 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 5: SGD for a two-layer neural network\n",
        "\n",
        "Now that you've written SGD for a linear classifier, let's consider the case of a simple two-layer neural network.  Specifically, for input $x \\in \\mathbb{R}^n$, we'll consider a two-layer neural network (without bias terms) of the form\n",
        "\\begin{equation}\n",
        "z = W_2^T \\mathrm{ReLU}(W_1^T x)\n",
        "\\end{equation}\n",
        "where $W_1 \\in \\mathbb{R}^{n \\times d}$ and $W_2 \\in \\mathbb{R}^{d \\times k}$ represent the weights of the network (which has a $d$-dimensional hidden unit), and where $z \\in \\mathbb{R}^k$ represents the logits output by the network.  We again use the softmax / cross-entropy loss, meaning that we want to solve the optimization problem\n",
        "\\begin{equation}\n",
        "\\minimize_{W_1, W_2} \\;\\; \\frac{1}{m} \\sum_{i=1}^m \\ell_{\\mathrm{softmax}}(W_2^T \\mathrm{ReLU}(W_1^T x^{(i)}), y^{(i)}).\n",
        "\\end{equation}\n",
        "Or alternatively, overloading the notation to describe the batch form with matrix $X \\in \\mathbb{R}^{m \\times n}$, this can also be written\n",
        "\\begin{equation}\n",
        "\\minimize_{W_1, W_2} \\;\\; \\ell_{\\mathrm{softmax}}(\\mathrm{ReLU}(X W_1) W_2, y).\n",
        "\\end{equation}\n",
        "\n",
        "Using the chain rule, we can derive the backpropagation updates for this network (we'll briefly cover these in class, on 9/8, but also provide the final form here for ease of implementation).  Specifically, let\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "Z_1 \\in \\mathbb{R}^{m \\times d} & = \\mathrm{ReLU}(X W_1) \\\\\n",
        "G_2 \\in \\mathbb{R}^{m \\times k} & = \\normalize(\\exp(Z_1 W_2)) - I_y \\\\\n",
        "G_1 \\in \\mathbb{R}^{m \\times d} & = \\mathrm{1}\\{Z_1 > 0\\} \\circ (G_2 W_2^T)\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "where $\\mathrm{1}\\{Z_1 > 0\\}$ is a binary matrix with entries equal to zero or one depending on whether each term in $Z_1$ is strictly positive and where $\\circ$ denotes elementwise multiplication.  Then the gradients of the objective are given by\n",
        "\\begin{equation}\n",
        "\\begin{split}\n",
        "\\nabla_{W_1} \\ell_{\\mathrm{softmax}}(\\mathrm{ReLU}(X W_1) W_2, y) & = \\frac{1}{m} X^T G_1  \\\\\n",
        "\\nabla_{W_2} \\ell_{\\mathrm{softmax}}(\\mathrm{ReLU}(X W_1) W_2, y) & = \\frac{1}{m} Z_1^T G_2.  \\\\\n",
        "\\end{split}\n",
        "\\end{equation}\n",
        "\n",
        "**Note:** If the details of these precise equations seem a bit cryptic to you (prior to the 9/8 lecture), don't worry too much.  These _are_ just the standard backpropagation equations for a two-layer ReLU network: the $Z_1$ term just computes the \"forward\" pass while the $G_2$ and $G_1$ terms denote the backward pass.  But the precise form of the updates can vary depending upon the notation you've used for neural networks, the precise ways you formulate the losses, if you've derived these previously in matrix form, etc.  If the notation seems like it might be familiar from when you've seen deep networks in the past, and makes more sense after the 9/8 lecture, that is more than sufficient in terms of background (after all, the whole _point_ of deep learning systems, to some extent, is that we don't need to bother with these manual calculations).  But if these entire concepts are _completely_ foreign to you, then it may be better to take a separate course on ML and neural networks prior to this course, or at least be aware that there will be substantial catch-up work to do for the course.\n",
        "\n",
        "Using these gradients, now write the `nn_epoch()` function in the `src/simple_ml.py` file.  As with the previous question, your solution should modify the `W1` and `W2` arrays in place.  After implementing the function, run the following test.  Be sure to use matrix operations as indicated by the expresssions above to implement the function: this will be _much_ faster, and more efficient, than attempting to use loops (and it requires far less code)."
      ],
      "metadata": {
        "id": "wpotma3NZsN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 -m pytest -k \"nn_epoch\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I59g_G3FZv_c",
        "outputId": "6b4affae-acfd-4d09-a02a-5ff02597ad6b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[32m.\u001b[0m\u001b[32m                                                [100%]\u001b[0m\n",
            "\n",
            "\u001b[32m======================= \u001b[32m\u001b[1m1 passed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[32m in 3.55s\u001b[0m\u001b[32m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a full neural network\n",
        "\n",
        "As before, though it isn't a strict necessity to pass the autograder, it's rather fun to see how well you can use your neural network function to train an MNIST classifier.  Analogous to the softmax regression case, there is a `train_nn()` function in the `simple_ml.py` file you can use to train this two-layer network via SGD with multiple epochs.  Here is code, for example, that trains a two-layer network with 400 hidden units."
      ],
      "metadata": {
        "id": "AjExyPuCegw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "\n",
        "# Reload the simple_ml module which has been cached from the earlier experiment\n",
        "import importlib\n",
        "import simple_ml\n",
        "importlib.reload(simple_ml)\n",
        "\n",
        "sys.path.append(\"src/\")\n",
        "from simple_ml import train_nn, parse_mnist\n",
        "\n",
        "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
        "                         \"data/train-labels-idx1-ubyte.gz\")\n",
        "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
        "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
        "train_nn(X_tr, y_tr, X_te, y_te, hidden_dim=400, epochs=20, lr=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltLd86nyehw5",
        "outputId": "911dbe47-e69f-46a5-9ebf-15c1d0024bae"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
            "|     0 |    0.15324 |   0.04697 |   0.16305 |  0.04920 |\n",
            "|     1 |    0.09854 |   0.02923 |   0.11604 |  0.03660 |\n",
            "|     2 |    0.07434 |   0.02167 |   0.09790 |  0.03160 |\n",
            "|     3 |    0.05947 |   0.01713 |   0.08773 |  0.02890 |\n",
            "|     4 |    0.04826 |   0.01355 |   0.08048 |  0.02610 |\n",
            "|     5 |    0.04048 |   0.01093 |   0.07690 |  0.02390 |\n",
            "|     6 |    0.03476 |   0.00895 |   0.07435 |  0.02320 |\n",
            "|     7 |    0.03026 |   0.00777 |   0.07256 |  0.02290 |\n",
            "|     8 |    0.02664 |   0.00650 |   0.07137 |  0.02200 |\n",
            "|     9 |    0.02355 |   0.00560 |   0.07004 |  0.02130 |\n",
            "|    10 |    0.02099 |   0.00463 |   0.06890 |  0.02080 |\n",
            "|    11 |    0.01908 |   0.00392 |   0.06852 |  0.02070 |\n",
            "|    12 |    0.01720 |   0.00322 |   0.06801 |  0.02070 |\n",
            "|    13 |    0.01561 |   0.00275 |   0.06746 |  0.02110 |\n",
            "|    14 |    0.01422 |   0.00238 |   0.06691 |  0.02070 |\n",
            "|    15 |    0.01297 |   0.00212 |   0.06644 |  0.02040 |\n",
            "|    16 |    0.01188 |   0.00178 |   0.06617 |  0.02030 |\n",
            "|    17 |    0.01085 |   0.00142 |   0.06560 |  0.01980 |\n",
            "|    18 |    0.01005 |   0.00122 |   0.06527 |  0.01900 |\n",
            "|    19 |    0.00921 |   0.00105 |   0.06492 |  0.01900 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 6: Softmax regression in C++\n",
        "\n",
        "The final question in this homework requires you to implement the same function you did in Question 4, a function that runs a single epoch of linear softmax regression.  But here, you will do so with a C++ implementation, rather than a Python one. (Strictly speaking, the actual implementation here is more like raw C, but we use C++ features to build the interface to Python using the [pybind11](https://pybind11.readthedocs.io) library, which you will also use to interface between C++ and Python in later homeworks.  Although there are other alternatives, pybind11 library is relatively nice as an interface, as it is a header-only library, and allows you to implement the entire Python/C++ interface within a single C++ source library.)\n",
        "\n",
        "The C++ file you'll implement things in is the `src/simple_ml_ext.cpp` file.  Let's take a look at the relevant portion of the file.  You will specifically implement your code in the following function\n",
        "\n",
        "```cpp\n",
        "void softmax_regression_epoch_cpp(const float *X, const unsigned char *y,\n",
        "\t\t\t\t\t\t\t\t  float *theta, size_t m, size_t n, size_t k,\n",
        "\t\t\t\t\t\t\t\t  float lr, size_t batch)\n",
        "{\n",
        "    /**\n",
        "     * A C++ version of the softmax regression epoch code.  This should run a\n",
        "     * single epoch over the data defined by X and y (and sizes m,n,k), and\n",
        "     * modify theta in place.  Your function will probably want to allocate\n",
        "     * (and then delete) some helper arrays to store the logits and gradients.\n",
        "     *\n",
        "     * Args:\n",
        "     *     X (const float *): pointer to X data, of size m*n, stored in row\n",
        "     *          major (C) format\n",
        "     *     y (const unsigned char *): pointer to y data, of size m\n",
        "     *     theta (float *): pointer to theta data, of size n*k, stored in row\n",
        "     *          major (C) format\n",
        "     *     m (size_t): number of examples\n",
        "     *     n (size_t): input dimension\n",
        "     *     k (size_t): number of classes\n",
        "     *     lr (float): learning rate / SGD step size\n",
        "     *     batch (int): SGD minibatch size\n",
        "     *\n",
        "     * Returns:\n",
        "     *     (None)\n",
        "     */\n",
        "\n",
        "    /// YOUR CODE HERE\n",
        "\n",
        "    /// END YOUR CODE\n",
        "}\n",
        "```\n",
        "Let's unpack the arguments to this function a bit.  The function essentially mirrors that of the Python implementation, but requires passing some additional arguments because we are operating on raw pointers to the array data rather than any sort of higher-level \"matrix\" data structure.  Specifically, `X`, `y`, and `theta` are pointers to the raw data of the corresponding numpy arrays from the previous section; for 2D arrays, these are stored in C-style (row-major) format, meaning that the first row of $X$ is stored sequentially as the first bytes in `X`, then the second row, etc (this contrasts with _column major_ ordering, which stores the first _column_ of the matrirx sequentially, then the second column, etc).  We also assuming there is no padding in the data; that is, the second row begins immediately after the first row, with no additional bytes added, e.g., to align the memory to a certain boundary (all these issues will be mentioned in subsequent discussion in the course, but avoided for now). Of course, because only the raw data is passed into the function, in order to know the actual sizes of the underlying matrices, we also need to pass these sizes explicitly to the function, which is what is provided by the `m`, `n`, and `k` arguments.\n",
        "\n",
        "As an illustration of how to access the data, note that because `X` represents a row-major, $m \\times n$ matrix, if we want to access the $(i,j)$ element of $X$ (the element in the $i$th row and the $j$th column), we would use the index\n",
        "```cpp\n",
        "X[i*n + j]\n",
        "```\n",
        "i.e., because `X` is $n$ columns, and stores it's columns sequentially, we'd need to access the `X[i*n]` element to access the $i$th row; and if we want to access the $j$th element in this row, we have the expression above.  The same logic would apply to the `theta` matrix, but importantly, because `theta` is a $n \\times k$ matrix, to access it's $(i,j)$ element you would use the index\n",
        "```cpp\n",
        "theta[i*k + j]\n",
        "```\n",
        "Unlike in Python, you need to be very careful when accessing memory directly like this in C++, and get very used to this kind of notation (or build additional data structures that help you access things in a more intuitive fashion, but for this assignment you should just stick to the raw indexing).\n",
        "\n",
        "\n",
        "The second piece of importance for the implementation is the pybind11 code that actually provides the Python interface\n",
        "```cpp\n",
        "PYBIND11_MODULE(simple_ml_ext, m) {\n",
        "    m.def(\"softmax_regression_epoch_cpp\",\n",
        "    \t[](py::array_t<float, py::array::c_style> X,\n",
        "           py::array_t<unsigned char, py::array::c_style> y,\n",
        "           py::array_t<float, py::array::c_style> theta,\n",
        "           float lr,\n",
        "           int batch) {\n",
        "        softmax_regression_epoch_cpp(\n",
        "        \tstatic_cast<const float*>(X.request().ptr),\n",
        "            static_cast<const unsigned char*>(y.request().ptr),\n",
        "            static_cast<float*>(theta.request().ptr),\n",
        "            X.request().shape[0],\n",
        "            X.request().shape[1],\n",
        "            theta.request().shape[1],\n",
        "            lr,\n",
        "            batch\n",
        "           );\n",
        "    },\n",
        "    py::arg(\"X\"), py::arg(\"y\"), py::arg(\"theta\"),\n",
        "    py::arg(\"lr\"), py::arg(\"batch\"));\n",
        "}\n",
        "```\n",
        "This code is provided for you in the file, and you should not change it at all.  But for those who are curious, this code essentially just extracts the raw pointers from the provided inputs (using pybinds numpy interface), and then calls the corresponding `softmax_regression_epoch_cpp` function.\n",
        "\n",
        "Using all this as background, implement the `softmax_regression_epoch_cpp` to accomplish the same updates as your Python implementation did.  Note that because you are just accessing the raw data, you will need to perform all the matrix-vector products manually, rather that rely on numpy to do all the matrix operations for you (**note: do not use an external matrix library like Eigen for this assignment, but code the multiplication yourself ... it is a relatively simple one**). After you do so, you can test the implementation using the following commands."
      ],
      "metadata": {
        "id": "ZoDlF1E0TUZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!make\n",
        "!python3 -m pytest -k \"softmax_regression_epoch_cpp\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISlt4nL_5I2N",
        "outputId": "6810cc1a-3c4a-4772-8755-96b689deee24"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c++ -O3 -Wall -shared -std=c++11 -fPIC $(python3 -m pybind11 --includes) src/simple_ml_ext.cpp -o src/simple_ml_ext.so\n",
            "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
            "platform linux -- Python 3.10.12, pytest-7.2.2, pluggy-1.0.0\n",
            "rootdir: /content/drive/MyDrive/10714/hw0\n",
            "plugins: anyio-3.6.2\n",
            "collected 6 items / 5 deselected / 1 selected                                  \u001b[0m\n",
            "\n",
            "tests/test_simple_ml.py \u001b[31mF\u001b[0m\u001b[31m                                                [100%]\u001b[0m\n",
            "\n",
            "=================================== FAILURES ===================================\n",
            "\u001b[31m\u001b[1m______________________ test_softmax_regression_epoch_cpp _______________________\u001b[0m\n",
            "\n",
            "    \u001b[94mdef\u001b[39;49;00m \u001b[92mtest_softmax_regression_epoch_cpp\u001b[39;49;00m():\u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# test numeical gradient\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        np.random.seed(\u001b[94m0\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        X = np.random.randn(\u001b[94m50\u001b[39;49;00m,\u001b[94m5\u001b[39;49;00m).astype(np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        y = np.random.randint(\u001b[94m3\u001b[39;49;00m, size=(\u001b[94m50\u001b[39;49;00m,)).astype(np.uint8)\u001b[90m\u001b[39;49;00m\n",
            "        Theta = np.zeros((\u001b[94m5\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m), dtype=np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        dTheta = -nd.Gradient(\u001b[94mlambda\u001b[39;49;00m Th : softmax_loss(X\u001b[37m@Th\u001b[39;49;00m.reshape(\u001b[94m5\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m),y))(Theta)\u001b[90m\u001b[39;49;00m\n",
            "        softmax_regression_epoch_cpp(X,y,Theta,lr=\u001b[94m1.0\u001b[39;49;00m,batch=\u001b[94m50\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        np.testing.assert_allclose(dTheta.reshape(\u001b[94m5\u001b[39;49;00m,\u001b[94m3\u001b[39;49;00m), Theta, rtol=\u001b[94m1e-4\u001b[39;49;00m, atol=\u001b[94m1e-4\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "    \u001b[90m\u001b[39;49;00m\n",
            "        \u001b[90m# test multi-steps on MNIST\u001b[39;49;00m\u001b[90m\u001b[39;49;00m\n",
            "        X,y = parse_mnist(\u001b[33m\"\u001b[39;49;00m\u001b[33mdata/train-images-idx3-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                          \u001b[33m\"\u001b[39;49;00m\u001b[33mdata/train-labels-idx1-ubyte.gz\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "        theta = np.zeros((X.shape[\u001b[94m1\u001b[39;49;00m], y.max()+\u001b[94m1\u001b[39;49;00m), dtype=np.float32)\u001b[90m\u001b[39;49;00m\n",
            "        softmax_regression_epoch_cpp(X[:\u001b[94m100\u001b[39;49;00m], y[:\u001b[94m100\u001b[39;49;00m], theta, lr=\u001b[94m0.1\u001b[39;49;00m, batch=\u001b[94m10\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            ">       np.testing.assert_allclose(np.linalg.norm(theta), \u001b[94m1.0947356\u001b[39;49;00m,\u001b[90m\u001b[39;49;00m\n",
            "                                   rtol=\u001b[94m1e-5\u001b[39;49;00m, atol=\u001b[94m1e-5\u001b[39;49;00m)\u001b[90m\u001b[39;49;00m\n",
            "\u001b[1m\u001b[31mE       AssertionError: \u001b[0m\n",
            "\u001b[1m\u001b[31mE       Not equal to tolerance rtol=1e-05, atol=1e-05\u001b[0m\n",
            "\u001b[1m\u001b[31mE       \u001b[0m\n",
            "\u001b[1m\u001b[31mE       Mismatched elements: 1 / 1 (100%)\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Max absolute difference: 0.00265548\u001b[0m\n",
            "\u001b[1m\u001b[31mE       Max relative difference: 0.00242569\u001b[0m\n",
            "\u001b[1m\u001b[31mE        x: array(1.09208, dtype=float32)\u001b[0m\n",
            "\u001b[1m\u001b[31mE        y: array(1.094736)\u001b[0m\n",
            "\n",
            "\u001b[1m\u001b[31mtests/test_simple_ml.py\u001b[0m:190: AssertionError\n",
            "\u001b[36m\u001b[1m=========================== short test summary info ============================\u001b[0m\n",
            "\u001b[31mFAILED\u001b[0m tests/test_simple_ml.py::\u001b[1mtest_softmax_regression_epoch_cpp\u001b[0m - AssertionError: \n",
            "\u001b[31m======================= \u001b[31m\u001b[1m1 failed\u001b[0m, \u001b[33m5 deselected\u001b[0m\u001b[31m in 1.25s\u001b[0m\u001b[31m ========================\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training a full softmax regression classifier with the C++ version\n",
        "\n",
        "Let's finally try training the whole softmax regression classifier using our \"direct memory acesss\" C++ version.  If the previous Python version took ~3 seconds, this should be blazing fast, right?"
      ],
      "metadata": {
        "id": "YavSmuA2T_in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"src/\")\n",
        "\n",
        "# Reload the simple_ml module to include the newly-compiled C++ extension\n",
        "import importlib\n",
        "import simple_ml\n",
        "importlib.reload(simple_ml)\n",
        "\n",
        "from simple_ml import train_softmax, parse_mnist\n",
        "\n",
        "X_tr, y_tr = parse_mnist(\"data/train-images-idx3-ubyte.gz\",\n",
        "                         \"data/train-labels-idx1-ubyte.gz\")\n",
        "X_te, y_te = parse_mnist(\"data/t10k-images-idx3-ubyte.gz\",\n",
        "                         \"data/t10k-labels-idx1-ubyte.gz\")\n",
        "\n",
        "train_softmax(X_tr, y_tr, X_te, y_te, epochs=10, lr = 0.2, batch=100, cpp=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPyMe1lDejIK",
        "outputId": "b456eaa7-a505-40fa-dbb1-895b8ee02a59"
      },
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Epoch | Train Loss | Train Err | Test Loss | Test Err |\n",
            "|     0 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     1 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     2 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     3 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     4 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     5 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     6 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     7 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     8 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n",
            "|     9 |    2.30259 |   0.90128 |   2.30259 |  0.90200 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the numbers match exactly our Python version, and the code is ... about 5 times slower?!  What is going on here?  Well, it turns out that the \"manual\" matrix multiplication code you probably wrote for the C++ version is extremely inefficient.  While Python itself is a slow, interpreted language, numpy itself is backed by matrix multiplications written in C (or, more likely, Fortran, believe it or not), that have been highly optmized to make use of vector operations, the cache hierarchy of different processors, and other features that are essential for efficient numerical operations.  We will cover these details much more in later lectures, and you'll even write a matrix library that can actually perform these operations relatively efficiently (at least for some special cases ... it's honestly not that easy to beat numpy in general).\n",
        "\n",
        "But for now, assuming your code recreates the Python behavior, you're done with the assignment, and can get ready for our next dive into automatic differentiation."
      ],
      "metadata": {
        "id": "UQmKcVE0UDRp"
      }
    }
  ]
}